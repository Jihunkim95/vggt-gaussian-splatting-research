#!/usr/bin/env python3
"""
Phase 1: Surrogate Rewardë¡œ RL Agent í•™ìŠµ (Multi-video Training)

ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµìœ¼ë¡œ zero-shot generalization ë‹¬ì„±
"""
import argparse
from pathlib import Path
import sys
sys.path.append('..')

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from env import FrameSelectionEnv
from utils.dataset_loader import load_dataset, create_train_val_split


def main():
    parser = argparse.ArgumentParser(description='RL Frame Selector - Phase 1 Training (Multi-video)')

    # Dataset selection
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['co3d', 'dtu', 'custom'],
                       help='í•™ìŠµ ë°ì´í„°ì…‹: co3d (CO3Dv2), dtu (DTU scans), custom (ì‚¬ìš©ì ë¹„ë””ì˜¤)')
    parser.add_argument('--num-train-videos', type=int, default=100,
                       help='í•™ìŠµì— ì‚¬ìš©í•  ë¹„ë””ì˜¤ ìˆ˜ (default: 100)')
    parser.add_argument('--num-eval-videos', type=int, default=10,
                       help='í‰ê°€ì— ì‚¬ìš©í•  ë¹„ë””ì˜¤ ìˆ˜ (default: 10)')
    parser.add_argument('--dataset-root', type=str, default=None,
                       help='ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (None = ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)')

    # Frame selection parameters
    parser.add_argument('--num-source-frames', type=int, default=300,
                       help='ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜ (default: 300)')
    parser.add_argument('--target-frames', type=int, default=60,
                       help='ìµœì¢… ì„ íƒí•  í”„ë ˆì„ ìˆ˜ (default: 60)')
    parser.add_argument('--max-gap', type=int, default=10,
                       help='ì—°ì† ì„ íƒ í”„ë ˆì„ ê°„ ìµœëŒ€ gap (overlap ë³´ì¥, default: 10)')

    # Training parameters
    parser.add_argument('--total-timesteps', type=int, default=500000,
                       help='ì´ í•™ìŠµ timesteps (default: 500000)')
    parser.add_argument('--n-envs', type=int, default=4,
                       help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (default: 4)')
    parser.add_argument('--output-dir', type=str, default='./trained_models',
                       help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (default: ./trained_models)')
    parser.add_argument('--tensorboard-log', type=str, default='./logs',
                       help='Tensorboard ë¡œê·¸ ë””ë ‰í† ë¦¬ (default: ./logs)')

    args = parser.parse_args()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 70)
    print("ğŸ¯ RL Frame Selector - Phase 1: Multi-Video Training")
    print("=" * 70)
    print(f"ğŸ“¦ Dataset: {args.dataset}")
    print(f"ğŸ“¹ Train videos: {args.num_train_videos}")
    print(f"ğŸ“¹ Eval videos: {args.num_eval_videos}")
    print(f"ğŸ¯ Source frames: {args.num_source_frames}")
    print(f"ğŸ¯ Target frames: {args.target_frames}")
    print(f"ğŸ“Š Total timesteps: {args.total_timesteps}")
    print(f"ğŸ”„ Parallel envs: {args.n_envs}")
    print("=" * 70)
    print()

    # Load videos from dataset
    print(f"ğŸ“¥ Loading {args.dataset} dataset...")
    kwargs = {}
    if args.dataset_root:
        kwargs['root'] = args.dataset_root

    all_videos = load_dataset(
        args.dataset,
        num_videos=args.num_train_videos + args.num_eval_videos,
        **kwargs
    )

    # Split into train/val
    split_data = create_train_val_split(
        all_videos,
        val_ratio=args.num_eval_videos / len(all_videos)
    )

    train_videos = split_data['train'][:args.num_train_videos]
    eval_videos = split_data['val'][:args.num_eval_videos]

    print(f"âœ… Loaded: {len(train_videos)} train, {len(eval_videos)} eval videos")
    print(f"ğŸ“ Sample train video: {train_videos[0]}")
    print()

    # Create multi-video training environment
    # ê° episodeë§ˆë‹¤ ëœë¤í•˜ê²Œ ë¹„ë””ì˜¤ ì„ íƒ
    import random
    import numpy as np

    def make_env(video_list, rank, seed=0):
        """
        í™˜ê²½ ìƒì„± í•¨ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ í˜¸ì¶œ)
        episodeë§ˆë‹¤ ëœë¤í•˜ê²Œ ë‹¤ë¥¸ ë¹„ë””ì˜¤ ì„ íƒ
        """
        def _init():
            # ëœë¤í•˜ê²Œ ë¹„ë””ì˜¤ ì„ íƒ
            video_path = random.choice(video_list)
            env = FrameSelectionEnv(
                video_path=video_path,
                num_source_frames=args.num_source_frames,
                target_frames=args.target_frames,
                max_gap=args.max_gap
            )
            env.reset(seed=seed + rank)
            return env
        return _init

    # Training í™˜ê²½ (ë³‘ë ¬ í™˜ê²½)
    print(f"ğŸ”§ Creating {args.n_envs} parallel training environments...")
    env = SubprocVecEnv([make_env(train_videos, i) for i in range(args.n_envs)])

    # Evaluation í™˜ê²½ (ë‹¨ì¼)
    print(f"ğŸ”§ Creating evaluation environment...")
    eval_env = FrameSelectionEnv(
        video_path=eval_videos[0],
        num_source_frames=args.num_source_frames,
        target_frames=args.target_frames,
        max_gap=args.max_gap
    )
    print()

    # PPO Agent ìƒì„±
    print("ğŸ¤– PPO Agent ì´ˆê¸°í™” ì¤‘...")
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=args.tensorboard_log,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01
    )

    print("âœ… Agent ìƒì„± ì™„ë£Œ")
    print()

    # Callbacks
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(output_dir / 'best_model'),
        log_path=str(output_dir / 'eval_logs'),
        eval_freq=5000,
        deterministic=True,
        render=False
    )

    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path=str(output_dir / 'checkpoints'),
        name_prefix='rl_frame_selector'
    )

    # í•™ìŠµ ì‹œì‘
    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    print(f"   Tensorboard: tensorboard --logdir {args.tensorboard_log}")
    print()

    model.learn(
        total_timesteps=args.total_timesteps,
        callback=[eval_callback, checkpoint_callback],
        progress_bar=True
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = output_dir / 'final_model.zip'
    model.save(final_model_path)

    print()
    print("=" * 70)
    print("âœ… Phase 1 Multi-Video Training ì™„ë£Œ!")
    print("=" * 70)
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥: {final_model_path}")
    print(f"ğŸ“ Best ëª¨ë¸: {output_dir / 'best_model'}")
    print()
    print("ğŸ“Š í•™ìŠµ í†µê³„:")
    print(f"   - ì´ í•™ìŠµ ë¹„ë””ì˜¤: {len(train_videos)}ê°œ")
    print(f"   - ì´ timesteps: {args.total_timesteps}")
    print(f"   - ë³‘ë ¬ í™˜ê²½: {args.n_envs}")
    print()
    print("ğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. Tensorboard í™•ì¸: tensorboard --logdir {args.tensorboard_log}")
    print(f"   2. Zero-shot í…ŒìŠ¤íŠ¸: python ../evaluate.py --model {final_model_path} --video <new_video>")
    print(f"   3. Phase 2 Fine-tuning ì§„í–‰ (ì‹¤ì œ 3DGS í‰ê°€)")
    print()


if __name__ == '__main__':
    main()
