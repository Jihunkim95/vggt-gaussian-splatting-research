# ğŸš€ Quick Start Guide

RL Frame Selectorë¥¼ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ëŠ” ê°€ì´ë“œ

## Step 1: í™˜ê²½ ì„¤ì • (1ë¶„)

```bash
# vggt_env í™œì„±í™”
cd /data/vggt-gaussian-splatting-research
source ./env/vggt_env/bin/activate

# RL íŒ¨í‚¤ì§€ ì„¤ì¹˜
cd rl_frame_selector
pip install -r requirements_rl.txt
```

## Step 2: Phase 1 Multi-Video í•™ìŠµ ì‹œì‘

### Option A: CO3Dv2 (ê¶Œì¥ - Zero-shot Generalization)

```bash
cd phase1_surrogate

# CO3Dv2 1,000ê°œ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ (1-2ì‹œê°„)
python train.py \
    --dataset co3d \
    --num-train-videos 1000 \
    --num-eval-videos 100 \
    --total-timesteps 500000 \
    --n-envs 8 \
    --output-dir ./trained_models/co3d_1k \
    --tensorboard-log ./logs/co3d_1k
```

### Option B: DTU (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸, 30ë¶„-1ì‹œê°„)

```bash
# DTU 10ê°œ ìŠ¤ìº”ìœ¼ë¡œ í•™ìŠµ
python train.py \
    --dataset dtu \
    --num-train-videos 10 \
    --num-eval-videos 2 \
    --total-timesteps 100000 \
    --n-envs 4 \
    --output-dir ./trained_models/dtu \
    --tensorboard-log ./logs/dtu
```

### Option C: Custom Videos

```bash
# ì‚¬ìš©ì ë¹„ë””ì˜¤ë¡œ í•™ìŠµ
python train.py \
    --dataset custom \
    --num-train-videos 20 \
    --num-eval-videos 3 \
    --total-timesteps 200000 \
    --n-envs 4 \
    --output-dir ./trained_models/custom \
    --tensorboard-log ./logs/custom
```

## Step 3: í•™ìŠµ ëª¨ë‹ˆí„°ë§

**ìƒˆ í„°ë¯¸ë„ ì—´ê¸°**:
```bash
cd /data/vggt-gaussian-splatting-research/rl_frame_selector/phase1_surrogate
tensorboard --logdir ./logs
```

ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸ (í¬íŠ¸í¬ì›Œë”© í•„ìš”):
```
http://localhost:6006
```

### ì£¼ìš” ë©”íŠ¸ë¦­

- `rollout/ep_rew_mean`: Episode reward (0.5 â†’ 0.8+ ëª©í‘œ)
- `train/policy_loss`: Policy gradient loss
- `train/value_loss`: Value function loss

## Step 4: í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸

```python
from stable_baselines3 import PPO
from env import FrameSelectionEnv

# ëª¨ë¸ ë¡œë“œ
model = PPO.load("./trained_models/final_model.zip")

# í™˜ê²½ ìƒì„±
env = FrameSelectionEnv(
    video_path="../../datasets/custom/cLectern.mp4",
    num_source_frames=300,
    target_frames=60
)

# í”„ë ˆì„ ì„ íƒ
obs, _ = env.reset()
selected_frames = []

for _ in range(300):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)

    if action == 1:
        selected_frames.append(info['current_step'] - 1)

    if done or truncated:
        break

print(f"ì„ íƒëœ í”„ë ˆì„: {len(selected_frames)}ê°œ")
print(f"ìµœì¢… reward: {reward:.4f}")
print(f"í”„ë ˆì„ ì¸ë±ìŠ¤: {sorted(selected_frames)}")
```

## ì˜ˆìƒ ê²°ê³¼

### Phase 1 í•™ìŠµ ê³¡ì„ 

```
Episode 0-100:    Reward 0.3-0.5 (random exploration)
Episode 100-500:  Reward 0.5-0.7 (learning)
Episode 500+:     Reward 0.7-0.85 (convergence)
```

### Baseline ë¹„êµ (Surrogate Reward)

| Method | Temporal Uniformity | Avg Quality | Diversity | Total Reward |
|--------|---------------------|-------------|-----------|--------------|
| Random | 0.3 | 0.5 | 0.6 | 0.45 |
| Uniform | 0.9 | 0.4 | 0.3 | 0.52 |
| Quality-only | 0.2 | 0.9 | 0.4 | 0.50 |
| Stratified | 0.7 | 0.7 | 0.5 | 0.63 |
| **RL Agent** | **0.8** | **0.8** | **0.6** | **0.73** |

## ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# CPUë¡œ í•™ìŠµ (ëŠë¦¬ì§€ë§Œ ê°€ëŠ¥)
python train.py --videos video.mp4 --total-timesteps 50000
```

### OpenCV ì—ëŸ¬
```bash
pip install opencv-python-headless
```

### Tensorboard í¬íŠ¸ ì¶©ëŒ
```bash
tensorboard --logdir ./logs --port 6007
```

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… Phase 1 ì™„ë£Œ â†’ Surrogate rewardë¡œ agent í•™ìŠµ
2. ğŸ”§ Phase 2 êµ¬í˜„ â†’ `run_3dgs_pipeline()` í†µí•©
3. ğŸ“Š Baseline ë¹„êµ â†’ Random/Uniform/Stratified
4. ğŸ“ ë³´ê³ ì„œ ì‘ì„± â†’ ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬

## ì°¸ê³ 

- ì „ì²´ ë¬¸ì„œ: [README.md](README.md)
- Phase 2 êµ¬í˜„: [phase2_finetune/finetune.py](phase2_finetune/finetune.py)
- í™˜ê²½ êµ¬í˜„: [phase1_surrogate/env.py](phase1_surrogate/env.py)
