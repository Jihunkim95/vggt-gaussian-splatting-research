
--- ÌéòÏù¥ÏßÄ 1 ---
VGGT: Visual Geometry Grounded Transformer Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2 Christian Rupprecht1 David Novotny2 1Visual Geometry Group, University of Oxford 2Meta AI ‚Ä¶ Figure 1. VGGT is a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data. It accepts up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which often outperforms optimization-based alternatives without further processing. Abstract We present VGGT, a feed-forward neural network that di- rectly infers all key 3D attributes of a scene, including cam- era parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and special- ized for single tasks. It is also simple and efficient, re- constructing images in under one second, and still out- performing alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, in- cluding camera parameter estimation, multi-view depth es- timation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt. 1. Introduction We consider the problem of estimating the 3D attributes of a scene, captured in a set of images, utilizing a feed- forward neural network. Traditionally, 3D reconstruction has been approached with visual-geometry methods, uti- lizing iterative optimization techniques like Bundle Adjust- ment (BA) [45]. Machine learning has often played an im- portant complementary role, addressing tasks that cannot be solved by geometry alone, such as feature matching and monocular depth prediction. The integration has become increasingly tight, and now state-of-the-art Structure-from- Motion (SfM) methods like VGGSfM [125] combine ma- chine learning and visual geometry end-to-end via differ- entiable BA. Even so, visual geometry still plays a major role in 3D reconstruction, which increases complexity and computational cost. As networks become ever more powerful, we ask if, finally, 3D tasks can be solved directly by a neural net- work, eschewing geometry post-processing almost entirely. Recent contributions like DUSt3R [129] and its evolution 1 arXiv:2503.11651v1 [cs.CV] 14 Mar 2025

--- ÌéòÏù¥ÏßÄ 2 ---
MASt3R [62] have shown promising results in this direc- tion, but these networks can only process two images at once and rely on post-processing to reconstruct more im- ages, fusing pairwise reconstructions. In this paper, we take a further step towards removing the need to optimize 3D geometry in post-processing. We do so by introducing Visual Geometry Grounded Transformer (VGGT), a feed-forward neural network that performs 3D reconstruction from one, a few, or even hundreds of input views of a scene. VGGT predicts a full set of 3D attributes, including camera parameters, depth maps, point maps, and 3D point tracks. It does so in a single forward pass, in sec- onds. Remarkably, it often outperforms optimization-based alternatives even without further processing. This is a sub- stantial departure from DUSt3R, MASt3R, or VGGSfM, which still require costly iterative post-optimization to ob- tain usable results. We also show that it is unnecessary to design a special network for 3D reconstruction. Instead, VGGT is based on a fairly standard large transformer [119], with no par- ticular 3D or other inductive biases (except for alternating between frame-wise and global attention), but trained on a large number of publicly available datasets with 3D annota- tions. VGGT is thus built in the same mold as large models for natural language processing and computer vision, such as GPTs [1, 29, 148], CLIP [86], DINO [10, 78], and Stable Diffusion [34]. These have emerged as versatile backbones that can be fine-tuned to solve new, specific tasks. Simi- larly, we show that the features computed by VGGT can significantly enhance downstream tasks like point tracking in dynamic videos, and novel view synthesis. There are several recent examples of large 3D neural net- works, including DepthAnything [142], MoGe [128], and LRM [49]. However, these models only focus on a sin- gle 3D task, such as monocular depth estimation or novel view synthesis. In contrast, VGGT uses a shared backbone to predict all 3D quantities of interest together. We demon- strate that learning to predict these interrelated 3D attributes enhances overall accuracy despite potential redundancies. At the same time, we show that, during inference, we can derive the point maps from separately predicted depth and camera parameters, obtaining better accuracy compared to directly using the dedicated point map head. To summarize, we make the following contributions: (1) We introduce VGGT, a large feed-forward transformer that, given one, a few, or even hundreds of images of a scene, can predict all its key 3D attributes, including camera intrin- sics and extrinsics, point maps, depth maps, and 3D point tracks, in seconds. (2) We demonstrate that VGGT‚Äôs pre- dictions are directly usable, being highly competitive and usually better than those of state-of-the-art methods that use slow post-processing optimization techniques. (3) We also show that, when further combined with BA post-processing, VGGT achieves state-of-the-art results across the board, even when compared to methods that specialize in a sub- set of 3D tasks, often improving quality substantially. We make our code and models publicly available at https://github.com/facebookresearch/vggt. We believe that this will facilitate further research in this direction and bene- fit the computer vision community by providing a new foun- dation for fast, reliable, and versatile 3D reconstruction. 2. Related Work Structure from Motion is a classic computer vision prob- lem [45, 77, 80] that involves estimating camera parameters and reconstructing sparse point clouds from a set of images of a static scene captured from different viewpoints. The traditional SfM pipeline [2, 36, 70, 94, 103, 134] consists of multiple stages, including image matching, triangulation, and bundle adjustment. COLMAP [94] is the most popu- lar framework based on the traditional pipeline. In recent years, deep learning has improved many components of the SfM pipeline, with keypoint detection [21, 31, 116, 149] and image matching [11, 67, 92, 99] being two primary ar- eas of focus. Recent methods [5, 102, 109, 112, 113, 118, 122, 125, 131, 160] explored end-to-end differentiable SfM, where VGGSfM [125] started to outperform traditional al- gorithms on challenging phototourism scenarios. Multi-view Stereo aims to densely reconstruct the geome- try of a scene from multiple overlapping images, typically assuming known camera parameters, which are often esti- mated with SfM. MVS methods can be divided into three categories: traditional handcrafted [38, 39, 96, 130], global optimization [37, 74, 133, 147], and learning-based meth- ods [42, 72, 84, 145, 157]. As in SfM, learning-based MVS approaches have recently seen a lot of progress. Here, DUSt3R [129] and MASt3R [62] directly estimate aligned dense point clouds from a pair of views, similar to MVS but without requiring camera parameters. Some concurrent works [111, 127, 141, 156] explore replacing DUSt3R‚Äôs test-time optimization with neural networks, though these attempts achieve only suboptimal or comparable perfor- mance to DUSt3R. Instead, VGGT outperforms DUSt3R and MASt3R by a large margin. Tracking-Any-Point was first introduced in Particle Video [91] and revived by PIPs [44] during the deep learn- ing era, aiming to track points of interest across video se- quences including dynamic motions. Given a video and some 2D query points, the task is to predict 2D correspon- dences of these points in all other frames. TAP-Vid [23] proposed three benchmarks for this task and a simple base- line method later improved in TAPIR [24]. CoTracker [55, 56] utilized correlations between different points to track through occlusions, while DOT [60] enabled dense track- ing through occlusions. Recently, TAPTR [63] proposed 2

--- ÌéòÏù¥ÏßÄ 3 ---
DINO Add camera token Frame Attention Global Attention Camera Head DPT Point maps Tracks Cameras Depth maps √ó ùêø times Input Concat Figure 2. Architecture Overview. Our model first patchifies the input images into tokens by DINO, and appends camera tokens for camera prediction. It then alternates between frame-wise and global self attention layers. A camera head makes the final prediction for camera extrinsics and intrinsics, and a DPT [87] head for any dense output. an end-to-end transformer for this task, and LocoTrack [13] extended commonly used pointwise features to nearby re- gions. All of these methods are specialized point trackers. Here, we demonstrate that VGGT‚Äôs features yield state-of- the-art tracking performance when coupled with existing point trackers. 3. Method We introduce VGGT, a large transformer that ingests a set of images as input and produces a variety of 3D quantities as output. We start by introducing the problem in Sec. 3.1, followed by our architecture in Sec. 3.2 and its prediction heads in Sec. 3.3, and finally the training setup in Sec. 3.4. 3.1. Problem definition and notation The input is a sequence (Ii)N i=1 of N RGB images Ii ‚àà R3√óH√óW , observing the same 3D scene. VGGT‚Äôs trans- former is a function that maps this sequence to a corre- sponding set of 3D annotations, one per frame: f (Ii)N i=1 = (gi, Di, Pi, Ti)N i=1 . (1) The transformer thus maps each image Ii to its camera pa- rameters gi ‚ààR9 (intrinsics and extrinsics), its depth map Di ‚ààRH√óW , its point map Pi ‚ààR3√óH√óW , and a grid Ti ‚ààRC√óH√óW of C-dimensional features for point track- ing. We explain next how these are defined. For the camera parameters gi, we use the parametriza- tion from [125] and set g = [q, t, f] which is the concatena- tion of the rotation quaternion q ‚ààR4, the translation vec- tor t ‚ààR3, and the field of view f ‚ààR2. We assume that the camera‚Äôs principal point is at the image center, which is common in SfM frameworks [95, 125]. We denote the domain of the image Ii with I(Ii) = {1, . . . , H} √ó {1, . . . , W}, i.e., the set of pixel locations. The depth map Di associates each pixel location y ‚ààI(Ii) with its corresponding depth value Di(y) ‚ààR+, as ob- served from the i-th camera. Likewise, the point map Pi associates each pixel with its corresponding 3D scene point Pi(y) ‚ààR3. Importantly, like in DUSt3R [129], the point maps are viewpoint invariant, meaning that the 3D points Pi(y) are defined in the coordinate system of the first cam- era g1, which we take as the world reference frame. Finally, for keypoint tracking, we follow track-any- point methods such as [25, 57]. Namely, given a fixed query image point yq in the query image Iq, the network outputs a track T ‚ãÜ(yq) = (yi)N i=1 formed by the corresponding 2D points yi ‚ààR2 in all images Ii. Note that the transformer f above does not output the tracks directly but instead features Ti ‚ààRC√óH√óW , which are used for tracking. The tracking is delegated to a sep- arate module, described in Sec. 3.3, which implements a function T ((yj)M j=1, (Ti)N i=1) = ((ÀÜyj,i)N i=1)M j=1. It ingests the query point yq and the dense tracking features Ti output by the transformer f and then computes the track. The two networks f and T are trained jointly end-to-end. Order of Predictions. The order of the images in the input sequence is arbitrary, except that the first image is chosen as the reference frame. The network architecture is designed to be permutation equivariant for all but the first frame. Over-complete Predictions. Notably, not all quantities predicted by VGGT are independent. For example, as shown by DUSt3R [129], the camera parameters g can be inferred from the invariant point map P, for instance, by solving the Perspective-n-Point (PnP) problem [35, 61]. 3

--- ÌéòÏù¥ÏßÄ 4 ---
Ours Input Single view Two views < 0.1s < 0.1s < 0.1s < 0.1s DUSt3R ‚Ä¶ ‚Ä¶ 32 views > 200s < 0.6 s Figure 3. Qualitative comparison of our predicted 3D points to DUSt3R on in-the-wild images. As shown in the top row, our method successfully predicts the geometric structure of an oil painting, while DUSt3R predicts a slightly distorted plane. In the second row, our method correctly recovers a 3D scene from two images with no overlap, while DUSt3R fails. The third row provides a challenging example with repeated textures, while our prediction is still high-quality. We do not include examples with more than 32 frames, as DUSt3R runs out of memory beyond this limit. Furthermore, the depth maps can be deduced from the point map and the camera parameters. However, as we show in Sec. 4.5, tasking VGGT with explicitly predicting all afore- mentioned quantities during training brings substantial per- formance gains, even when these are related by closed-form relationships. Meanwhile, during inference, it is observed that combining independently estimated depth maps and camera parameters produces more accurate 3D points com- pared to directly employing a specialized point map branch. 3.2. Feature Backbone Following recent works in 3D deep learning [53, 129, 132], we design a simple architecture with minimal 3D induc- tive biases, letting the model learn from ample quantities of 3D-annotated data. In particular, we implement the model f as a large transformer [119]. To this end, each input image I is initially patchified into a set of K tokens1 tI ‚ààRK√óC through DINO [78]. The combined set of im- age tokens from all frames, i.e., tI = ‚à™N i=1{tI i }, is subse- quently processed through the main network structure, al- ternating frame-wise and global self-attention layers. Alternating-Attention. We slightly adjust the standard transformer design by introducing Alternating-Attention 1The number of tokens depends on the image resolution. (AA), making the transformer focus within each frame and globally in an alternate fashion. Specifically, frame-wise self-attention attends to the tokens tI k within each frame separately, and global self-attention attends to the tokens tI across all frames jointly. This strikes a balance between integrating information across different images and normal- izing the activations for the tokens within each image. By default, we employ L = 24 layers of global and frame-wise attention. In Sec. 4, we demonstrate that our AA architec- ture brings significant performance gains. Note that our ar- chitecture does not employ any cross-attention layers, only self-attention ones. 3.3. Prediction heads Here, we describe how f predicts the camera parameters, depth maps, point maps, and point tracks. First, for each input image Ii, we augment the corresponding image to- kens tI i with an additional camera token tg i ‚ààR1√óC‚Ä≤ and four register tokens [19] tR i ‚ààR4√óC‚Ä≤. The concatenation of (tI i , tg i , tR i j)N i=1 is then passed to the AA transformer, yield- ing output tokens (ÀÜtI i ,ÀÜtg i ,ÀÜtR i )N i=1. Here, the camera token and register tokens of the first frame (tg 1 := ¬Øtg, tR 1 := ¬ØtR) are set to a different set of learnable tokens t g, t R than those of all other frames (tg i := t g, tR i := t R, i ‚àà[2, . . . , N]), 4

--- ÌéòÏù¥ÏßÄ 5 ---
Figure 4. Additional Visualizations of Point Map Estimation. Camera frustums illustrate the estimated camera poses. Explore our interactive demo for better visualization quality. which are also learnable. This allows the model to distin- guish the first frame from the rest, and to represent the 3D predictions in the coordinate frame of the first camera. Note that the refined camera and register tokens now become frame-specific‚Äî‚Äìthis is because our AA transformer con- tains frame-wise self-attention layers that allow the trans- former to match the camera and register tokens with the cor- responding tokens from the same image. Following com- mon practice, the output register tokens ÀÜtR i are discarded while ÀÜtI i , ÀÜtg i are used for prediction. Coordinate Frame. As noted above, we predict cameras, point maps, and depth maps in the coordinate frame of the first camera g1. As such, the camera extrinsics output for the first camera are set to the identity, i.e., the first rotation quaternion is q1 = [0, 0, 0, 1] and the first translation vector is t1 = [0, 0, 0]. Recall that the special camera and register tokens tg 1 := t g, tR 1 := t R allow the transformer to identify the first camera. Camera Predictions. The camera parameters (ÀÜgi)N i=1 are predicted from the output camera tokens (ÀÜtg i )N i=1 using four additional self-attention layers followed by a linear layer. This forms the camera head that predicts the camera intrin- sics and extrinsics. Dense Predictions. The output image tokens ÀÜtI i are used to predict the dense outputs, i.e., the depth maps Di, point maps Pi, and tracking features Ti. More specifically, ÀÜtI i are first converted to dense feature maps Fi ‚ààRC‚Ä≤‚Ä≤√óH√óW with a DPT layer [87]. Each Fi is then mapped with a 3 √ó 3 con- volutional layer to the corresponding depth and point maps Di and Pi. Additionally, the DPT head also outputs dense features Ti ‚ààRC√óH√óW , which serve as input to the track- ing head. We also predict the aleatoric uncertainty [58, 76] Œ£D i ‚ààRH√óW + and Œ£P i ‚ààRH√óW + for each depth and point map, respectively. As described in Sec. 3.4, the uncertainty maps are used in the loss and, after training, are proportional to the model‚Äôs confidence in the predictions. Tracking. In order to implement the tracking module T , we use the CoTracker2 architecture [57], which takes the dense tracking features Ti as input. More specifically, given a query point yj in a query image Iq (during training, we al- ways set q = 1, but any other image can be potentially used as a query), the tracking head T predicts the set of 2D points T ((yj)M j=1, (Ti)N i=1) = ((ÀÜyj,i)N i=1)M j=1 in all images Ii that correspond to the same 3D point as y. To do so, the fea- ture map Tq of the query image is first bilinearly sampled at 5

--- ÌéòÏù¥ÏßÄ 6 ---
the query point yj to obtain its feature. This feature is then correlated with all other feature maps Ti, i Ã∏= q to obtain a set of correlation maps. These maps are then processed by self-attention layers to predict the final 2D points ÀÜyi, which are all in correspondence with yj. Note that, similar to VG- GSfM [125], our tracker does not assume any temporal or- dering of the input frames and, hence, can be applied to any set of input images, not just videos. 3.4. Training Training Losses. We train the VGGT model f end-to-end using a multi-task loss: L = Lcamera + Ldepth + Lpmap + ŒªLtrack. (2) We found that the camera (Lcamera), depth (Ldepth), and point-map (Lpmap) losses have similar ranges and do not need to be weighted against each other. The tracking loss Ltrack is down-weighted with a factor of Œª = 0.05. We de- scribe each loss term in turn. The camera loss Lcamera supervises the cameras ÀÜg: Lcamera = PN i=1 ‚à•ÀÜgi ‚àígi‚à•œµ , comparing the predicted cam- eras ÀÜgi with the ground truth gi using the Huber loss | ¬∑ |œµ. The depth loss Ldepth follows DUSt3R [129] and im- plements the aleatoric-uncertainty loss [59, 75] weighing the discrepancy between the predicted depth ÀÜDi and the ground-truth depth Di with the predicted uncertainty map ÀÜŒ£D i . Differently from DUSt3R, we also apply a gradient- based term, which is widely used in monocular depth es- timation. Hence, the depth loss is Ldepth = PN i=1 ‚à•Œ£D i ‚äô ( ÀÜDi ‚àíDi)‚à•+ ‚à•Œ£D i ‚äô(‚àáÀÜDi ‚àí‚àáDi)‚à•‚àíŒ± log Œ£D i , where ‚äô is the channel-broadcast element-wise product. The point map loss is defined analogously but with the point-map un- certainty Œ£P i : Lpmap = PN i=1 ‚à•Œ£P i ‚äô( ÀÜPi ‚àíPi)‚à•+ ‚à•Œ£P i ‚äô (‚àáÀÜPi ‚àí‚àáPi)‚à•‚àíŒ± log Œ£P i . Finally, the tracking loss is given by Ltrack = PM j=1 PN i=1 ‚à•yj,i ‚àíÀÜyj,i‚à•. Here, the outer sum runs over all ground-truth query points yj in the query image Iq, yj,i is yj‚Äôs ground-truth correspondence in image Ii, and ÀÜyj,i is the corresponding prediction obtained by the application T ((yj)M j=1, (Ti)N i=1) of the tracking module. Additionally, following CoTracker2 [57], we apply a visibility loss (bi- nary cross-entropy) to estimate whether a point is visible in a given frame. Ground Truth Coordinate Normalization. If we scale a scene or change its global reference frame, the images of the scene are not affected at all, meaning that any such variant is a legitimate result of 3D reconstruction. We remove this ambiguity by normalizing the data, thus making a canoni- cal choice and task the transformer to output this particular variant. We follow [129] and, first, express all quantities in the coordinate frame of the first camera g1. Then, we com- pute the average Euclidean distance of all 3D points in the point map P to the origin and use this scale to normalize the camera translations t, the point map P, and the depth map D. Importantly, unlike [129], we do not apply such normalization to the predictions output by the transformer; instead, we force it to learn the normalization we choose from the training data. Implementation Details. By default, we employ L = 24 layers of global and frame-wise attention, respectively. The model consists of approximately 1.2 billion parameters in total. We train the model by optimizing the training loss (2) with the AdamW optimizer for 160K iterations. We use a cosine learning rate scheduler with a peak learning rate of 0.0002 and a warmup of 8K iterations. For every batch, we randomly sample 2‚Äì24 frames from a random training scene. The input frames, depth maps, and point maps are re- sized to a maximum dimension of 518 pixels. The aspect ra- tio is randomized between 0.33 and 1.0. We also randomly apply color jittering, Gaussian blur, and grayscale augmen- tation to the frames. The training runs on 64 A100 GPUs over nine days. We employ gradient norm clipping with a threshold of 1.0 to ensure training stability. We leverage bfloat16 precision and gradient checkpointing to improve GPU memory and computational efficiency. Training Data. The model was trained using a large and diverse collection of datasets, including: Co3Dv2 [88], BlendMVS [146], DL3DV [69], MegaDepth [64], Kubric [41], WildRGB [135], ScanNet [18], Hyper- Sim [89], Mapillary [71], Habitat [107], Replica [104], MVS-Synth [50], PointOdyssey [159], Virtual KITTI [7], Aria Synthetic Environments [82], Aria Digital Twin [82], and a synthetic dataset of artist-created assets similar to Objaverse [20]. These datasets span various domains, including indoor and outdoor environments, and encompass synthetic and real-world scenarios. The 3D annotations for these datasets are derived from multiple sources, such as direct sensor capture, synthetic engines, or SfM techniques [95]. The combination of our datasets is broadly comparable to those of MASt3R [30] in size and diversity. 4. Experiments This section compares our method to state-of-the-art ap- proaches across multiple tasks to show its effectiveness. 4.1. Camera Pose Estimation We first evaluate our method on the CO3Dv2 [88] and RealEstate10K [161] datasets for camera pose estimation, as shown in Tab. 1. Following [124], we randomly select 10 images per scene and evaluate them using the standard met- ric AUC@30, which combines RRA and RTA. RRA (Rela- tive Rotation Accuracy) and RTA (Relative Translation Ac- curacy) calculate the relative angular errors in rotation and translation, respectively, for each image pair. These angu- 6

--- ÌéòÏù¥ÏßÄ 7 ---
Methods Re10K (unseen) CO3Dv2 Time AUC@30 ‚Üë AUC@30 ‚Üë Colmap+SPSG [92] 45.2 25.3 ‚àº15s PixSfM [66] 49.4 30.1 > 20s PoseDiff [124] 48.0 66.5 ‚àº7s DUSt3R [129] 67.7 76.7 ‚àº7s MASt3R [62] 76.4 81.8 ‚àº9s VGGSfM v2 [125] 78.9 83.4 ‚àº10s MV-DUSt3R [111] ‚Ä° 71.3 69.5 ‚àº0.6s CUT3R [127] ‚Ä° 75.3 82.8 ‚àº0.6s FLARE [156] ‚Ä° 78.8 83.3 ‚àº0.5s Fast3R [141] ‚Ä° 72.7 82.5 ‚àº0.2s Ours (Feed-Forward) 85.3 88.2 ‚àº0.2s Ours (with BA) 93.5 91.8 ‚àº1.8s Table 1. Camera Pose Estimation on RealEstate10K [161] and CO3Dv2 [88] with 10 random frames. All metrics the higher the better. None of the methods were trained on the Re10K dataset. Runtime were measured using one H100 GPU. Methods marked with ‚Ä° represent concurrent work. Known GT Method Acc.‚Üì Comp.‚Üì Overall‚Üì camera ‚úì Gipuma [40] 0.283 0.873 0.578 ‚úì MVSNet [144] 0.396 0.527 0.462 ‚úì CIDER [139] 0.417 0.437 0.427 ‚úì PatchmatchNet [121] 0.427 0.377 0.417 ‚úì MASt3R [62] 0.403 0.344 0.374 ‚úì GeoMVSNet [157] 0.331 0.259 0.295 ‚úó DUSt3R [129] 2.677 0.805 1.741 ‚úó Ours 0.389 0.374 0.382 Table 2. Dense MVS Estimation on the DTU [51] Dataset. Methods operating with known ground-truth camera are in the top part of the table, while the bottom part contains the methods that do not know the ground-truth camera. Methods Acc.‚Üì Comp.‚Üì Overall‚Üì Time DUSt3R 1.167 0.842 1.005 ‚àº7s MASt3R 0.968 0.684 0.826 ‚àº9s Ours (Point) 0.901 0.518 0.709 ‚àº0.2s Ours (Depth + Cam) 0.873 0.482 0.677 ‚àº0.2s Table 3. Point Map Estimation on ETH3D [97]. DUSt3R and MASt3R use global alignment while ours is feed-forward and, hence, much faster. The row Ours (Point) indicates the results using the point map head directly, while Ours (Depth + Cam) de- notes constructing point clouds from the depth map head com- bined with the camera head. lar errors are then thresholded to determine the accuracy scores. AUC is the area under the accuracy-threshold curve of the minimum values between RRA and RTA across vary- ing thresholds. The (learnable) methods in Tab. 1 have been trained on Co3Dv2 and not on RealEstate10K. Our feed- forward model consistently outperforms competing meth- Method AUC@5 ‚Üë AUC@10 ‚Üë AUC@20 ‚Üë SuperGlue [92] 16.2 33.8 51.8 LoFTR [105] 22.1 40.8 57.6 DKM [32] 29.4 50.7 68.3 CasMTR [9] 27.1 47.0 64.4 Roma [33] 31.8 53.4 70.9 Ours 33.9 55.2 73.4 Table 4. Two-View matching comparison on ScanNet-1500 [18, 92]. Although our tracking head is not specialized for the two- view setting, it outperforms the state-of-the-art two-view matching method Roma. Measured in AUC (higher is better). ods across all metrics on both datasets, including those that employ computationally expensive post-optimization steps, such as Global Alignment for DUSt3R/MASt3R and Bun- dle Adjustment for VGGSfM, typically requiring more than 10 seconds. In contrast, VGGT achieves superior perfor- mance while only operating in a feed-forward manner, re- quiring just 0.2 seconds on the same hardware. Compared to concurrent works [111, 127, 141, 156] (indicated by ‚Ä°), our method demonstrates significant performance advan- tages, with speed similar to the fastest variant Fast3R [141]. Furthermore, our model‚Äôs performance advantage is even more pronounced on the RealEstate10K dataset, which none of the methods presented in Tab. 1 were trained on. This validates the superior generalization of VGGT. Our results also show that VGGT can be improved even further by combining it with optimization methods from vi- sual geometry optimization like BA. Specifically, refining the predicted camera poses and tracks with BA further im- proves accuracy. Note that our method directly predicts close-to-accurate point/depth maps, which can serve as a good initialization for BA. This eliminates the need for tri- angulation and iterative refinement in BA as done by [125], making our approach significantly faster (only around 2 sec- onds even with BA). Hence, while the feed-forward mode of VGGT outperforms all previous alternatives (whether they are feed-forward or not), there is still room for improvement since post-optimization still brings benefits. 4.2. Multi-view Depth Estimation Following MASt3R [62], we further evaluate our multi- view depth estimation results on the DTU [51] dataset. We report the standard DTU metrics, including Accuracy (the smallest Euclidean distance from the prediction to ground truth), Completeness (the smallest Euclidean distance from the ground truth to prediction), and their average Overall (i.e., Chamfer distance). In Tab. 2, DUSt3R and our VGGT are the only two methods operating without the knowl- edge of ground truth cameras. MASt3R derives depth maps by triangulating matches using the ground truth cameras. Meanwhile, deep multi-view stereo methods like GeoMVS- 7

--- ÌéòÏù¥ÏßÄ 8 ---
Ours CoTracker +Ours Figure 5. Visualization of Rigid and Dynamic Point Tracking. Top: VGGT‚Äôs tracking module T outputs keypoint tracks for an unordered set of input images depicting a static scene. Bottom: We finetune the backbone of VGGT to enhance a dynamic point tracker CoTracker [56], which processes sequential inputs. Net use ground truth cameras to construct cost volumes. Our method substantially outperforms DUSt3R, reduc- ing the Overall score from 1.741 to 0.382. More impor- tantly, it achieves results comparable to methods that know ground-truth cameras at test time. The significant perfor- mance gains can likely be attributed to our model‚Äôs multi- image training scheme that teaches it to reason about multi- view triangulation natively, instead of relying on ad hoc alignment procedures, such as in DUSt3R, which only av- erages multiple pairwise camera triangulations. 4.3. Point Map Estimation We also compare the accuracy of our predicted point cloud to DUSt3R and MASt3R on the ETH3D [97] dataset. For each scene, we randomly sample 10 frames. The pre- dicted point cloud is aligned to the ground truth using the Umeyama [117] algorithm. The results are reported after filtering out invalid points using the official masks. We re- port Accuracy, Completeness, and Overall (Chamfer dis- tance) for point map estimation. As shown in Tab. 3, al- though DUSt3R and MASt3R conduct expensive optimiza- tion (global alignment‚Äì‚Äîaround 10 seconds per scene), our method still outperforms them significantly in a simple feed-forward regime at only 0.2 seconds per reconstruction. Meanwhile, compared to directly using our estimated point maps, we found that the predictions from our depth and camera heads (i.e., unprojecting the predicted depth maps to 3D using the predicted camera parameters) yield higher accuracy. We attribute this to the benefits of de- composing a complex task (point map estimation) into sim- pler subproblems (depth map and camera prediction), even though camera, depth maps, and point maps are jointly su- pervised during training. We present a qualitative comparison with DUSt3R on in- the-wild scenes in Fig. 3 and further examples in Fig. 4. VGGT outputs high-quality predictions and generalizes ETH3D Dataset Acc.‚Üì Comp.‚Üì Overall‚Üì Cross-Attention 1.287 0.835 1.061 Global Self-Attention Only 1.032 0.621 0.827 Alternating-Attention 0.901 0.518 0.709 Table 5. Ablation Study for Transformer Backbone on ETH3D. We compare our alternating-attention architecture against two variants: one using only global self-attention and another employ- ing cross-attention. well, excelling on challenging out-of-domain examples, such as oil paintings, non-overlapping frames, and scenes with repeating or homogeneous textures like deserts. 4.4. Image Matching Two-view image matching is a widely-explored topic [68, 93, 105] in computer vision. It represents a specific case of rigid point tracking, which is restricted to only two views, and hence a suitable evaluation benchmark to measure our tracking accuracy, even though our model is not specialized for this task. We follow the standard protocol [33, 93] on the ScanNet dataset [18] and report the results in Tab. 4. For each image pair, we extract the matches and use them to estimate an essential matrix, which is then decomposed to a relative camera pose. The final metric is the relative pose accuracy, measured by AUC. For evaluation, we use ALIKED [158] to detect keypoints, treating them as query points yq. These are then passed to our tracking branch T to find correspondences in the second frame. We adopt the evaluation hyperparameters (e.g., the number of matches, RANSAC thresholds) from Roma [33]. Despite not being explicitly trained for two-view matching, Tab. 4 shows that VGGT achieves the highest accuracy among all baselines. 4.5. Ablation Studies Feature Backbone. We first validate the effectiveness of our proposed Alternating-Attention design by comparing it 8

--- ÌéòÏù¥ÏßÄ 9 ---
w. Lcamera w. Ldepth w. Ltrack Acc.‚Üì Comp.‚Üì Overall‚Üì ‚úó ‚úì ‚úì 1.042 0.627 0.834 ‚úì ‚úó ‚úì 0.920 0.534 0.727 ‚úì ‚úì ‚úó 0.976 0.603 0.790 ‚úì ‚úì ‚úì 0.901 0.518 0.709 Table 6. Ablation Study for Multi-task Learning, which shows that simultaneous training with camera, depth and track estimation yields the highest accuracy in point map estimation on ETH3D. against two alternative attention architectures: (a) global self-attention only, and (b) cross-attention. To ensure a fair comparison, all model variants maintain an identical number of parameters, using a total of 2L attention lay- ers. For the cross-attention variant, each frame indepen- dently attends to tokens from all other frames, maximiz- ing cross-frame information fusion although significantly increasing the runtime, particularly as the number of input frames grows. The hyperparameters such as the hidden di- mension and the number of heads are kept the same. Point map estimation accuracy is chosen as the evaluation metric for our ablation study, as it reflects the model‚Äôs joint under- standing of scene geometry and camera parameters. Results in Tab. 5 demonstrate that our Alternating-Attention archi- tecture outperforms both baseline variants by a clear mar- gin. Additionally, our other preliminary exploratory exper- iments consistently showed that architectures using cross- attention generally underperform compared to those exclu- sively employing self-attention. Multi-task Learning. We also verify the benefit of train- ing a single network to simultaneously learn multiple 3D quantities, even though these outputs may potentially over- lap (e.g., depth maps and camera parameters together can produce point maps). As shown in Tab. 6, there is a no- ticeable decrease in the accuracy of point map estimation when training without camera, depth, or track estimation. Notably, incorporating camera parameter estimation clearly enhances point map accuracy, whereas depth estimation contributes only marginal improvements. 4.6. Finetuning for Downstream Tasks We now show that the VGGT pre-trained feature extractor can be reused in downstream tasks. We show this for feed- forward novel view synthesis and dynamic point tracking. Feed-forward Novel View Synthesis is progressing rapidly [8, 43, 49, 53, 108, 126, 140, 155]. Most exist- ing methods take images with known camera parameters as input and predict the target image corresponding to a new camera viewpoint. Instead of relying on an explicit 3D rep- resentation, we follow LVSM [53] and modify VGGT to di- rectly output the target image. However, we do not assume known camera parameters for the input frames. We follow the training and evaluation protocol of LVSM Input Images Ground Truth Prediction Figure 6. Qualitative Examples of Novel View Synthesis. The top row shows the input images, the middle row displays the ground truth images from target viewpoints, and the bottom row presents our synthesized images. Method Known Input Cam Size PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì LGM [110] ‚úì 256 21.44 0.832 0.122 GS-LRM [154] ‚úì 256 29.59 0.944 0.051 LVSM [53] ‚úì 256 31.71 0.957 0.027 Ours-NVS‚àó ‚úó 224 30.41 0.949 0.033 Table 7. Quantitative comparisons for view synthesis on GSO [28] dataset. Finetuning VGGT for feed-forward novel view synthesis, it demonstrates competitive performance even without knowing camera extrinsic and intrinsic parameters for the input images. Note that ‚àóindicates using a small training set (only 20%). closely, e.g., using 4 input views and adopting Pl¬®ucker rays to represent target viewpoints. We make a simple modifi- cation to VGGT. As before, the input images are converted into tokens by DINO. Then, for the target views, we use a convolutional layer to encode their Pl¬®ucker ray images into tokens. These tokens, representing both the input im- ages and the target views, are concatenated and processed by the AA transformer. Subsequently, a DPT head is used to regress the RGB colors for the target views. It is impor- tant to note that we do not input the Pl¬®ucker rays for the source images. Hence, the model is not given the camera parameters for these input frames. LVSM was trained on the Objaverse dataset [20]. We use a similar internal dataset of approximately 20% the size of Objaverse. Further details on training and evaluation can be found in [53]. As shown in Tab. 7, despite not requir- ing the input camera parameters and using less training data than LVSM, our model achieves competitive results on the GSO dataset [28]. We expect that better results would be obtained using a larger training dataset. Qualitative exam- ples are shown in Fig. 6. Dynamic Point Tracking has emerged as a highly com- petitive task in recent years [25, 44, 57, 136], and it serves as another downstream application for our learned features. Following standard practices, we report these point-tracking metrics: Occlusion Accuracy (OA), which comprises the bi- nary accuracy of occlusion predictions; Œ¥vis avg, comprising the 9

--- ÌéòÏù¥ÏßÄ 10 ---
Method Kinetics RGB-S DAVIS AJ Œ¥vis avg OA AJ Œ¥vis avg OA AJ Œ¥vis avg OA TAPTR [63] 49.0 64.4 85.2 60.8 76.2 87.0 63.0 76.1 91.1 LocoTrack [13] 52.9 66.8 85.3 69.7 83.2 89.5 62.9 75.3 87.2 BootsTAPIR [26] 54.6 68.4 86.5 70.8 83.0 89.9 61.4 73.6 88.7 CoTracker [56] 49.6 64.3 83.3 67.4 78.9 85.2 61.8 76.1 88.3 CoTracker + Ours 57.2 69.0 88.9 72.1 84.0 91.6 64.7 77.5 91.4 Table 8. Dynamic Point Tracking Results on the TAP-Vid benchmarks. Although our model was not designed for dynamic scenes, simply fine-tuning CoTracker with our pretrained weights significantly enhances performance, demonstrating the robustness and effectiveness of our learned features. mean proportion of visible points accurately tracked within a certain pixel threshold; and Average Jaccard (AJ), mea- suring tracking and occlusion prediction accuracy together. We adapt the state-of-the-art CoTracker2 model [57] by substituting its backbone with our pretrained feature back- bone. This is necessary because VGGT is trained on un- ordered image collections instead of sequential videos. Our backbone predicts the tracking features Ti, which replace the outputs of the feature extractor and later enter the rest of the CoTracker2 architecture, that finally predicts the tracks. We finetune the entire modified tracker on Kubric [41]. As illustrated in Tab. 8, the integration of pretrained VGGT sig- nificantly enhances CoTracker‚Äôs performance on the TAP- Vid benchmark [23]. For instance, VGGT‚Äôs tracking fea- tures improve the Œ¥vis avg metric from 78.9 to 84.0 on the TAP- Vid RGB-S dataset. Despite the TAP-Vid benchmark‚Äôs in- clusion of videos featuring rapid dynamic motions from var- ious data sources, our model‚Äôs strong performance demon- strates the generalization capability of its features, even in scenarios for which it was not explicitly designed. 5. Discussions Limitations. While our method exhibits strong generaliza- tion to diverse in-the-wild scenes, several limitations re- main. First, the current model does not support fisheye or panoramic images. Additionally, reconstruction perfor- mance drops under conditions involving extreme input ro- tations. Moreover, although our model handles scenes with minor non-rigid motions, it fails in scenarios involving sub- stantial non-rigid deformation. However, an important advantage of our approach is its flexibility and ease of adaptation. Addressing these limi- tations can be straightforwardly achieved by fine-tuning the model on targeted datasets with minimal architectural modi- fications. This adaptability clearly distinguishes our method from existing approaches, which typically require extensive re-engineering during test-time optimization to accommo- date such specialized scenarios. Input Frames 1 2 4 8 10 20 50 100 200 Time (s) 0.04 0.05 0.07 0.11 0.14 0.31 1.04 3.12 8.75 Mem. (GB) 1.88 2.07 2.45 3.23 3.63 5.58 11.41 21.15 40.63 Table 9. Runtime and peak GPU memory usage across differ- ent numbers of input frames. Runtime is measured in seconds, and GPU memory usage is reported in gigabytes. Runtime and Memory. As shown in Tab. 9, we evalu- ate inference runtime and peak GPU memory usage of the feature backbone when processing varying numbers of in- put frames. Measurements are conducted using a single NVIDIA H100 GPU with flash attention v3 [98]. Images have a resolution of 336 √ó 518. We focus on the cost associated with the feature back- bone since users may select different branch combinations depending on their specific requirements and available re- sources. The camera head is lightweight, typically account- ing for approximately 5% of the runtime and about 2% of the GPU memory used by the feature backbone. A DPT head uses an average of 0.03 seconds and 0.2 GB GPU memory per frame. When GPU memory is sufficient, multiple frames can be processed efficiently in a single forward pass. At the same time, in our model, inter-frame relationships are handled only within the feature backbone, and the DPT heads make independent predictions per frame. Therefore, users con- strained by GPU resources may perform predictions frame by frame. We leave this trade-off to the user‚Äôs discretion. We recognize that a naive implementation of global self- attention can be highly memory-intensive with a large num- ber of tokens. Savings or accelerations can be achieved by employing techniques used in large language model (LLM) deployments. For instance, Fast3R [141] employs Ten- sor Parallelism to accelerate inference with multiple GPUs, which can be directly applied to our model. Patchifying. As discussed in Sec. 3.2, we have explored the method of patchifying images into tokens by utilizing either a 14 √ó 14 convolutional layer or a pretrained DI- NOv2 model. Empirical results indicate that the DINOv2 model provides better performance; moreover, it ensures much more stable training, particularly in the initial stages. The DINOv2 model is also less sensitive to variations in hyperparameters such as learning rate or momentum. Con- sequently, we have chosen DINOv2 as the default method for patchifying in our model. Differentiable BA. We also explored the idea of using differentiable bundle adjustment as in VGGSfM [125]. In small-scale preliminary experiments, differentiable BA demonstrated promising performance. However, a bottle- neck is its computational cost during training. Enabling differentiable BA in PyTorch using Theseus [85] typically makes each training step roughly 4 times slower, which 10

--- ÌéòÏù¥ÏßÄ 11 ---
is expensive for large-scale training. While customizing a framework to expedite training could be a potential solu- tion, it falls outside the scope of this work. Thus, we opted not to include differentiable BA in this work, but we recog- nize it as a promising direction for large-scale unsupervised training, as it can serve as an effective supervision signal in scenarios lacking explicit 3D annotations. Single-view Reconstruction. Unlike systems like DUSt3R and MASt3R that have to duplicate an image to create a pair, our model architecture inherently supports the input of a single image. In this case, global attention simply transi- tions to frame-wise attention. Although our model was not explicitly trained for single-view reconstruction, it demon- strates surprisingly good results. Some examples can be found in Fig. 3 and Fig. 7. We strongly encourage trying our demo for better visualization. Normalizing Prediction. As discussed in Sec. 3.4, our ap- proach normalizes the ground truth using the average Eu- clidean distance of the 3D points. While some methods, such as DUSt3R, also apply such normalization to network predictions, our findings suggest that it is neither necessary for convergence nor advantageous for final model perfor- mance. Furthermore, it tends to introduce additional insta- bility during the training phase. 6. Conclusions We present Visual Geometry Grounded Transformer (VGGT), a feed-forward neural network that can directly estimate all key 3D scene properties for hundreds of in- put views. It achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi- view depth estimation, dense point cloud reconstruction, and 3D point tracking. Our simple, neural-first approach departs from traditional visual geometry-based methods, which rely on optimization and post-processing to obtain accurate and task-specific results. The simplicity and effi- ciency of our approach make it well-suited for real-time ap- plications, which is another benefit over optimization-based approaches. Appendix In the Appendix, we provide the following: ‚Ä¢ formal definitions of key terms in Appendix A. ‚Ä¢ comprehensive implementation details, including archi- tecture and training hyperparameters in Appendix B. ‚Ä¢ additional experiments and discussions in Appendix C. ‚Ä¢ qualitative examples of single-view reconstruction in Ap- pendix D. ‚Ä¢ an expanded review of related works in Appendix E. A. Formal Definitions In this section, we provide additional formal definitions that further ground the method section. The camera extrinsics are defined in relation to the world reference frame, which we take to be the coordinate system of the first camera. We thus introduce two functions. The first function Œ≥(g, p) = p‚Ä≤ applies the rigid transformation encoded by g to a point p in the world reference frame to obtain the corresponding point p‚Ä≤ in the camera reference frame. The second function œÄ(g, p) = y further applies perspective projection, mapping the 3D point p to a 2D im- age point y. We also denote the depth of the point as ob- served from the camera g by œÄD(g, p) = d ‚ààR+. We model the scene as a collection of regular surfaces Si ‚äÇR3. We make this a function of the i-th input image as the scene can change over time [151]. The depth at pixel location y ‚ààI(Ii) is defined as the minimum depth of any 3D point p in the scene that projects to y, i.e., Di(y) = min{œÄD(gi, p) : p ‚ààSi ‚àßœÄ(gi, p) = y}. The point at pixel location y is then given by Pi(y) = Œ≥(g, p), where p ‚ààSi is the 3D point that minimizes the expression above, i.e., p ‚ààSi ‚àßœÄ(gi, p) = y ‚àßœÄD(gi, p) = Di(y). B. Implementation Details Architecture. As mentioned in the main paper, VGGT consists of 24 attention blocks, each block equipped with one frame-wise self-attention layer and one global self- attention layer. Following the ViT-L model used in DI- NOv2 [78], each attention layer is configured with a feature dimension of 1024 and employs 16 heads. We use the of- ficial implementation of the attention layer from PyTorch, i.e., torch.nn.MultiheadAttention, with flash attention en- abled. To stabilize training, we also use QKNorm [48] and LayerScale [115] for each attention layer. The value of Lay- erScale is initialized with 0.01. For image tokenization, we use DINOv2 [78] and add positional embedding. As in [143], we feed the tokens from the 4-th, 11-th, 17-th, and 23-rd block into DPT [87] for upsampling. Training. To form a training batch, we first choose a ran- dom training dataset (each dataset has a different yet ap- proximately similar weight, as in [129]), and from the 11

--- ÌéòÏù¥ÏßÄ 12 ---
dataset, we then sample a random scene (uniformly). Dur- ing the training phase, we select between 2 and 24 frames per scene while maintaining the constant total of 48 frames within each batch. For training, we use the respective train- ing sets of each dataset. We exclude training sequences con- taining fewer than 24 frames. RGB frames, depth maps, and point maps are first isotropically resized, so the longer size has 518 pixels. Then, we crop the shorter dimen- sion (around the principal point) to a size between 168 and 518 pixels while remaining a multiple of the 14-pixel patch size. It is worth mentioning that we apply aggressive color augmentation independently across each frame within the same scene, enhancing the model‚Äôs robustness to varying lighting conditions. We build ground truth tracks follow- ing [33, 105, 125], which unprojects depth maps to 3D, reprojects points to target frames, and retains correspon- dences where reprojected depths match target depth maps. Frames with low similarity to the query frame are excluded during batch sampling. In rare cases with no valid corre- spondences, the tracking loss is omitted. C. Additional Experiments Camera Pose Estimation on IMC We also evaluate using the Image Matching Challenge (IMC) [54], a camera pose estimation benchmark focusing on phototourism data. Until recently, the benchmark was dominated by classical incre- mental SfM methods [94]. Baselines. We evaluate two flavors of our model: VGGT and VGGT + BA. VGGT directly outputs camera pose es- timates, while VGGT + BA refines the estimates using an additional Bundle Adjustment stage. We compare to the classical incremental SfM methods such as [66, 94] and to recently-proposed deep methods. Specifically, recently VGGSfM [125] provided the first end-to-end trained deep method that outperformed incremental SfM on the chal- lenging phototourism datasets. Besides VGGSfM, we additionally compare to recently popularized DUSt3R [129] and MASt3R [62]. It is im- portant to note that DUSt3R and MASt3R utilized a sub- stantial portion of the MegaDepth dataset for training, only excluding scenes 0015 and 0022. The MegaDepth scenes employed in their training have some overlap with the IMC benchmark, although the images are not identical; the same scenes are present in both datasets. For instance, the MegaDepth scene 0024 corresponds to the British Museum, while the British Museum is also a scene in the IMC bench- mark. For an apples-to-apples comparison, we adopt the same training split as DUSt3R and MASt3R. In the main paper, to ensure a fair comparison on ScanNet-1500, we ex- clude the corresponding ScanNet scenes from our training. Results. Table 10 contains the results of our evaluation. Al- though phototourism data is the traditional focus of SfM Method Test-time Opt. AUC@3‚ó¶ AUC@5‚ó¶ AUC@10‚ó¶ Runtime COLMAP (SIFT+NN) [94] ‚úì 23.58 32.66 44.79 >10s PixSfM (SIFT + NN) [66] ‚úì 25.54 34.80 46.73 >20s PixSfM (LoFTR) [66] ‚úì 44.06 56.16 69.61 >20s PixSfM (SP + SG) [66] ‚úì 45.19 57.22 70.47 >20s DFSfM (LoFTR) [47] ‚úì 46.55 58.74 72.19 >10s DUSt3R [129] ‚úì 13.46 21.24 35.62 ‚àº7s MASt3R [62] ‚úì 30.25 46.79 57.42 ‚àº9s VGGSfM [125] ‚úì 45.23 58.89 73.92 ‚àº6s VGGSfMv2 [125] ‚úì 59.32 67.78 76.82 ‚àº10s VGGT (ours) ‚úó 39.23 52.74 71.26 0.2s VGGT + BA (ours) ‚úì 66.37 75.16 84.91 1.8s Table 10. Camera Pose Estimation on IMC [54]. Our method achieves state-of-the-art performance on the challenging pho- totropism data, outperforming VGGSfMv2 [125] which ranked first on the latest CVPR‚Äô24 IMC Challenge in camera pose (ro- tation and translation) estimation. methods, our VGGT‚Äôs feed-forward performance is on par with the state-of-the-art VGGSfMv2 with AUC@10 of 71.26 versus 76.82, while being significantly faster (0.2 vs. 10 seconds per scene). Remarkably, VGGT outper- forms both MASt3R [62] and DUSt3R [129] significantly across all accuracy thresholds while being much faster. This is because MASt3R‚Äôs and DUSt3R‚Äôs feed-forward predic- tions can only process pairs of frames and, hence, require a costly global alignment step. Additionally, with bundle ad- justment, VGGT + BA further improves drastically, achiev- ing state-of-the-art performance on IMC, raising AUC@10 from 71.26 to 84.91, and raising AUC@3 from 39.23 to 66.37. Note that our model directly predicts 3D points, which can serve as the initialization for BA. This elimi- nates the need for triangulation and iterative refinement of BA as in [125]. As a result, VGGT + BA is much faster than [125]. D. Qualitative Examples We further present qualitative examples of single-view re- construction in Fig. 7. E. Related Work In this section, we discuss additional related works. Vision Transformers. The Transformer architecture was initially proposed for language processing tasks [6, 22, 120]. It was later introduced to the computer vision com- munity by ViT [27], sparking widespread adoption. Vision Transformers and their variants have since become domi- nant in the design of architectures for various computer vi- sion tasks [4, 12, 83, 137], thanks to their simplicity, high capacity, flexibility, and ability to capture long-range de- pendencies. DeiT [114] demonstrated that Vision Transformers can be effectively trained on datasets like ImageNet using strong data augmentation strategies. DINO [10] revealed 12

--- ÌéòÏù¥ÏßÄ 13 ---
Figure 7. Single-view Reconstruction by Point Map Estimation. Unlike DUSt3R, which requires duplicating an image into a pair, our model can predict the point map from a single input image. It demonstrates strong generalization to unseen real-world images. intriguing properties of features learned by Vision Trans- formers in a self-supervised manner. CaiT [115] introduced layer scaling to address the challenges of training deeper Vision Transformers, effectively mitigating gradient-related issues. Further, techniques such as QKNorm [48, 150] have been proposed to stabilize the training process. Addition- ally, [138] also explores the dynamics between frame-wise and global attention modules in object tracking, though us- ing cross-attention. Camera Pose Estimation. Estimating camera poses from multi-view images is a crucial problem in 3D computer vi- sion. Over the last decades, Structure from Motion (SfM) has emerged as the dominant approach [46], whether in- cremental [2, 36, 94, 103, 134] or global [3, 14‚Äì17, 52, 73, 79, 81, 90, 106]. Recently, a set of methods treat camera pose estimation as a regression problem [65, 100, 109, 112, 113, 118, 122, 123, 131, 152, 153, 160], which show promising results under the sparse-view setting. Ace- Zero [5] further proposes to regress 3D scene coordinates and FlowMap [101] focuses on depth maps, as intermedi- ates for camera prediction. Instead, VGGSfM [125] simpli- fies the classical SfM pipeline to a differentiable framework, demonstrating exceptional performance, particularly with phototourism datasets. At the same time, DUSt3R [62, 129] introduces an approach to learn pixel-aligned point map, and hence camera poses can be recovered by simple align- ment. This paradigm shift has garnered considerable inter- est as the point map, an over-parameterized representation, offers seamless integration with various downstream appli- cations, such as 3D Gaussian splatting. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day. Communications of the ACM, 54(10):105‚Äì112, 2011. 2, 13 [3] Mica Arie-Nachimson, Shahar Z Kovalsky, Ira Kemelmacher-Shlizerman, Amit Singer, and Ronen Basri. Global motion estimation from point matches. In 2012 Second international conference on 3D imaging, modeling, processing, visualization & transmission, pages 81‚Äì88. IEEE, 2012. 13 [4] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuÀáci¬¥c, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF inter- national conference on computer vision, pages 6836‚Äì6846, 2021. 12 [5] Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cav- allari, ¬¥Aron Monszpart, Daniyar Turmukhambetov, and Victor Adrian Prisacariu. Scene coordinate reconstruction: Posing of image collections via incremental learning of a relocalizer. In ECCV, 2024. 2, 13 [6] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 12 [7] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 6 [8] Ang Cao, Justin Johnson, Andrea Vedaldi, and David Novotny. Lightplane: Highly-scalable components for neu- ral 3Dfields. In Proceedings of the International Confer- ence on 3D Vision (3DV), 2025. 9 [9] Chenjie Cao and Yanwei Fu. Improving transformer-based image matching by cascaded capturing spatially informa- tive keypoints. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision (ICCV), pages 12129‚Äì12139, 2023. 7 [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv¬¥e J¬¥egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 13

--- ÌéòÏù¥ÏßÄ 14 ---
Emerging properties in self-supervised vision transformers. In Proc. ICCV, 2021. 2, 12 [11] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to match features with seeded graph matching network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6301‚Äì6310, 2021. 2 [12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan- der Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1290‚Äì1299, 2022. 12 [13] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seun- gryong Kim, and Joon-Young Lee. Local all-pair corre- spondence for point tracking. Proc. ECCV, 2024. 3, 10 [14] David J Crandall, Andrew Owens, Noah Snavely, and Daniel P Huttenlocher. Sfm with mrfs: Discrete-continuous optimization for large-scale structure from motion. IEEE transactions on pattern analysis and machine intelligence, 35(12):2841‚Äì2853, 2012. 13 [15] Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 1212‚Äì1221, 2017. [16] Zhaopeng Cui and Ping Tan. Global structure-from-motion by similarity averaging. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 864‚Äì872, 2015. [17] Zhaopeng Cui, Nianjuan Jiang, Chengzhou Tang, and Ping Tan. Linear global translation estimation with feature tracks. arXiv preprint arXiv:1503.01832, 2015. 13 [18] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nie√üner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828‚Äì5839, 2017. 6, 7, 8 [19] Timoth¬¥ee Darcet, Maxime Oquab, Julien Mairal, and Pi- otr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. 4 [20] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142‚Äì13153, 2023. 6, 9 [21] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224‚Äì236, 2018. 2 [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. 12 [23] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adri`a Re- casens, Lucas Smaira, Yusuf Aytar, JoÀúao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: A benchmark for track- ing any point in a video. arXiv, 2022. 2, 10 [24] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. TAPIR: Tracking any point with per-frame initialization and temporal refinement. arXiv, 2306.08637, 2023. 2 [25] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. TAPIR: tracking any point with per-frame initialization and temporal refinement. In Proc. CVPR, 2023. 3, 9 [26] Carl Doersch, Yi Yang, Dilara Gokay, Pauline Luc, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ross Goroshin, JoÀúao Carreira, and Andrew Zisserman. Bootstap: Boot- strapped training for tracking-any-point. arXiv preprint arXiv:2402.00847, 2024. 10 [27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im- age is worth 16√ó16 words: Transformers for image recog- nition at scale. In Proc. ICLR, 2021. 12 [28] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automa- tion (ICRA), pages 2553‚Äì2560. IEEE, 2022. 9 [29] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur¬¥elien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Chris- tian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Di- nan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Ander- son, Graeme Nail, Gr¬¥egoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, and Kevin Stone. The Llama 3 herd of models. arXiv, 2407.21783, 2024. 2 [30] Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud. 14

--- ÌéòÏù¥ÏßÄ 15 ---
MASt3R-SfM: a fully-integrated solution for unconstrained structure-from-motion. arXiv, 2409.19152, 2024. 6 [31] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle- feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2- net: A trainable cnn for joint description and detection of local features. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 8092‚Äì8101, 2019. 2 [32] Johan Edstedt, Ioannis Athanasiadis, MÀöarten Wadenb¬®ack, and Michael Felsberg. DKM: Dense kernelized feature matching for geometry estimation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 7 [33] Johan Edstedt, Qiyu Sun, Georg B¬®okman, MÀöarten Wadenb¬®ack, and Michael Felsberg. Roma: Robust dense feature matching. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 19790‚Äì19800, 2024. 7, 8, 12 [34] Patrick Esser, Robin Rombach, and Bj¬®orn Ommer. Taming transformers for high-resolution image synthesis. In Proc. CVPR, 2021. 2 [35] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communica- tions of the ACM, 24(6):381‚Äì395, 1981. 3 [36] Jan-Michael Frahm, Pierre Fite-Georgel, David Gallup, Tim Johnson, Rahul Raguram, Changchang Wu, Yi-Hung Jen, Enrique Dunn, Brian Clipp, Svetlana Lazebnik, et al. Building rome on a cloudless day. In Computer Vision‚Äì ECCV 2010: 11th European Conference on Computer Vi- sion, Heraklion, Crete, Greece, September 5-11, 2010, Pro- ceedings, Part IV 11, pages 368‚Äì381. Springer, 2010. 2, 13 [37] Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wen- bing Tao. Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. Advances in Neural Information Processing Systems, 35:3403‚Äì3416, 2022. 2 [38] Yasutaka Furukawa, Carlos Hern¬¥andez, et al. Multi-view stereo: A tutorial. Foundations and Trends¬Æ in Computer Graphics and Vision, 9(1-2):1‚Äì148, 2015. 2 [39] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In Proceedings of the IEEE international confer- ence on computer vision, pages 873‚Äì881, 2015. 2 [40] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, 2015. 7 [41] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra- gasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh- Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad- wan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. In Proc. CVPR, 2022. 6, 10 [42] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high- resolution multi-view stereo and stereo matching. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2495‚Äì2504, 2020. 2 [43] Junlin Han, Jianyuan Wang, Andrea Vedaldi, Philip Torr, and Filippos Kokkinos. Flex3d: Feed-forward 3d genera- tion with flexible reconstruction model and input view cu- ration. arXiv preprint arXiv:2410.00890, 2024. 9 [44] Adam W Harley, Zhaoyuan Fang, and Katerina Fragki- adaki. Particle video revisited: Tracking through occlusions using point trajectories. In Proc. ECCV, 2022. 2, 9 [45] Richard Hartley and Andrew Zisserman. Multiple View Ge- ometry in Computer Vision. Cambridge University Press, 2000. 1, 2 [46] Richard Hartley and Andrew Zisserman. Multiple View Ge- ometry in Computer Vision. Cambridge University Press, ISBN: 0521540518, 2004. 13 [47] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free struc- ture from motion. In arxiv, 2023. 12 [48] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. 11, 13 [49] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single im- age to 3D. In Proc. ICLR, 2024. 2, 9 [50] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 6 [51] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aan√¶s. Large scale multi-view stereopsis eval- uation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 406‚Äì413. IEEE, 2014. 7 [52] Nianjuan Jiang, Zhaopeng Cui, and Ping Tan. A global linear method for camera pose registration. In Proceedings of the IEEE international conference on computer vision, pages 481‚Äì488, 2013. 13 [53] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. LVSM: a large view synthesis model with minimal 3D inductive bias. arXiv, 2410.17242, 2024. 4, 9 [54] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Im- age matching across wide baselines: From paper to prac- tice. International Journal of Computer Vision, 129(2): 517‚Äì547, 2021. 12 [55] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Co- tracker3: Simpler and better point tracking by pseudo- labelling real videos. arXiv preprint arXiv:2410.11831, 2024. 2 [56] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Co- tracker: It is better to track together. Proc. ECCV, 2024. 2, 8, 10 15

--- ÌéòÏù¥ÏßÄ 16 ---
[57] Nikita Karaev, Ignacio Rocco, Ben Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Co- Tracker: It is better to track together. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 3, 5, 6, 9, 10 [58] Alex Kendall and Roberto Cipolla. Modelling uncertainty in deep learning for camera relocalization. In Proc. ICRA. IEEE, 2016. 5 [59] Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for computer vision? Proc. NeurIPS, 2017. 6 [60] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Dense optical tracking: Connecting the dots. In CVPR, 2024. 2 [61] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Ep n p: An accurate o (n) solution to the p n p problem. In- ternational journal of computer vision, 81:155‚Äì166, 2009. 3 [62] Vincent Leroy, Yohann Cabon, and J¬¥erÀÜome Revaud. Grounding image matching in 3d with mast3r. arXiv preprint arXiv:2406.09756, 2024. 2, 7, 12, 13 [63] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection. arXiv preprint arXiv:2403.13042, 2024. 2, 10 [64] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pages 2041‚Äì2050, 2018. 6 [65] Amy Lin, Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Relpose++: Recovering 6d poses from sparse- view observations. arXiv preprint arXiv:2305.04926, 2023. 13 [66] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-perfect structure-from-motion with featuremetric refinement. arXiv.cs, abs/2108.08291, 2021. 7, 12 [67] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. Lightglue: Local feature matching at light speed. arXiv preprint arXiv:2306.13643, 2023. 2 [68] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: local feature matching at light speed. In Proc. ICCV, 2023. 8 [69] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22160‚Äì22169, 2024. 6 [70] Shaohui Liu, Yidan Gao, Tianyi Zhang, R¬¥emi Pautrat, Jo- hannes L Sch¬®onberger, Viktor Larsson, and Marc Pollefeys. Robust incremental structure-from-motion with hybrid fea- tures. In European Conference on Computer Vision, pages 249‚Äì269. Springer, 2025. 2 [71] Manuel Lopez-Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bul ÀúA¬≤, Yubin Kuang, and Peter Kontschieder. Mapillary planet-scale depth dataset. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 6 [72] Zeyu Ma, Zachary Teed, and Jia Deng. Multiview stereo with cascaded epipolar raft. In European Conference on Computer Vision, pages 734‚Äì750. Springer, 2022. 2 [73] Pierre Moulon, Pascal Monasse, and Renaud Marlet. Global fusion of relative motions for robust, accurate and scalable structure from motion. In Proceedings of the IEEE international conference on computer vision, pages 3248‚Äì 3255, 2013. 13 [74] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervi- sion. In Proceedings of the IEEE/CVF conference on com- puter vision and pattern recognition, pages 3504‚Äì3515, 2020. 2 [75] David Novotn¬¥y, Diane Larlus, and Andrea Vedaldi. Learn- ing 3D object categories by looking around them. In Pro- ceedings of the International Conference on Computer Vi- sion (ICCV), 2017. 6 [76] David Novotn¬¥y, Diane Larlus, and Andrea Vedaldi. Captur- ing the geometry of object categories from video supervi- sion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. 5 [77] John Oliensis. A critique of structure-from-motion algo- rithms. Computer Vision and Image Understanding, 80(2): 172‚Äì214, 2000. 2 [78] Maxime Oquab, Timoth¬¥ee Darcet, Th¬¥eo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fer- nandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El- Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Ar- mand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 2, 4, 11 [79] Onur Ozyesil and Amit Singer. Robust camera location es- timation by convex programming. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 2674‚Äì2683, 2015. 13 [80] Onur ¬®Ozyes¬∏il, Vladislav Voroninski, Ronen Basri, and Amit Singer. A survey of structure from motion*. Acta Numerica, 26:305‚Äì364, 2017. 2 [81] Linfei Pan, Daniel Barath, Marc Pollefeys, and Jo- hannes Lutz Sch¬®onberger. Global Structure-from-Motion Revisited. In European Conference on Computer Vision (ECCV), 2024. 13 [82] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Pe- ters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng (Carl) Ren. Aria digital twin: A new benchmark dataset for egocentric 3d machine percep- tion. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV), pages 20133‚Äì20143, 2023. 6 [83] William Peebles and Saining Xie. Scalable diffusion mod- els with transformers. In Proceedings of the IEEE/CVF In- 16

--- ÌéòÏù¥ÏßÄ 17 ---
ternational Conference on Computer Vision, pages 4195‚Äì 4205, 2023. 12 [84] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi- view stereo: A unified representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8645‚Äì8654, 2022. 2 [85] Luis Pineda, Taosha Fan, Maurizio Monge, Shobha Venkataraman, Paloma Sodhi, Ricky TQ Chen, Joseph Or- tiz, Daniel DeTone, Austin Wang, Stuart Anderson, et al. Theseus: A library for differentiable nonlinear optimiza- tion. Advances in Neural Information Processing Systems, 35:3801‚Äì3818, 2022. 10 [86] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proc. ICML, pages 8748‚Äì8763, 2021. 2 [87] Ren¬¥e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vi- sion, pages 12179‚Äì12188, 2021. 3, 5, 11 [88] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Com- mon Objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction. In Proc. ICCV, 2021. 6, 7 [89] Mike Roberts, Jason Ramapuram, Anurag Ranjan, At- ulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: A photorealis- tic synthetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. 6 [90] Rother. Linear multiview reconstruction of points, lines, planes and cameras using a reference plane. In Proceedings Ninth IEEE International Conference on Computer Vision, pages 1210‚Äì1217. IEEE, 2003. 13 [91] Peter Sand and Seth Teller. Particle video: Long-range mo- tion estimation using point trajectories. IJCV, 80, 2008. 2 [92] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938‚Äì4947, 2020. 2, 7 [93] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: learning feature matching with graph neural networks. In Proc. CVPR, 2020. 8 [94] Johannes Lutz Sch¬®onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Com- puter Vision and Pattern Recognition (CVPR), 2016. 2, 12, 13 [95] Johannes Lutz Sch¬®onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proc. CVPR, 2016. 3, 6 [96] Johannes L Sch¬®onberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The Nether- lands, October 11-14, 2016, Proceedings, Part III 14, pages 501‚Äì518. Springer, 2016. 2 [97] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An- dreas Geiger. A multi-view stereo benchmark with high- resolution images and multi-camera videos. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 3260‚Äì3269, 2017. 7, 8 [98] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37: 68658‚Äì68685, 2024. 10 [99] Yan Shi, Jun-Xiong Cai, Yoli Shavit, Tai-Jiang Mu, Wensen Feng, and Kai Zhang. Clustergnn: Cluster-based coarse- to-fine graph neural network for efficient feature matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12517‚Äì12526, 2022. 2 [100] Samarth Sinha, Jason Y Zhang, Andrea Tagliasacchi, Igor Gilitschenski, and David B Lindell. Sparsepose: Sparse- view camera pose regression and refinement. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21349‚Äì21359, 2023. 13 [101] Cameron Smith, David Charatan, Ayush Tewari, and Vin- cent Sitzmann. Flowmap: High-quality camera poses, in- trinsics, and depth via gradient descent. arXiv preprint arXiv:2404.15259, 2024. 13 [102] Cameron Smith, David Charatan, Ayush Tewari, and Vin- cent Sitzmann. FlowMap: high-quality camera poses, in- trinsics, and depth via gradient descent. arXiv, 2404.15259, 2024. 2 [103] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In ACM sig- graph 2006 papers, pages 835‚Äì846. 2006. 2, 13 [104] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur- Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 6 [105] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 8922‚Äì8931, 2021. 7, 8, 12 [106] Chris Sweeney, Torsten Sattler, Tobias Hollerer, Matthew Turk, and Marc Pollefeys. Optimizing the viewing graph for structure-from-motion. In Proceedings of the IEEE in- ternational conference on computer vision, pages 801‚Äì809, 2015. 13 [107] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wi- jmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt 17

--- ÌéòÏù¥ÏßÄ 18 ---
Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to re- arrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 6 [108] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d recon- struction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10208‚Äì 10217, 2024. 9 [109] Chengzhou Tang and Ping Tan. Ba-net: Dense bundle ad- justment network. arXiv preprint arXiv:1806.04807, 2018. 2, 13 [110] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi- view gaussian model for high-resolution 3d content cre- ation. In European Conference on Computer Vision, pages 1‚Äì18. Springer, 2024. 9 [111] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. 2, 7 [112] Zachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure from motion. arXiv preprint arXiv:1812.04605, 2018. 2, 13 [113] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:16558‚Äì16569, 2021. 2, 13 [114] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv¬¥e J¬¥egou. Training data-efficient image transformers & distillation through at- tention. In International conference on machine learning, pages 10347‚Äì10357. PMLR, 2021. 12 [115] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv¬¥e J¬¥egou. Going deeper with image transformers. In Proceedings of the IEEE/CVF in- ternational conference on computer vision, pages 32‚Äì42, 2021. 11, 13 [116] Micha≈Ç Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk: Learning local features with policy gradient. Advances in Neural Information Processing Systems, 33:14254‚Äì14265, 2020. 2 [117] Shinji Umeyama. Least-squares estimation of transforma- tion parameters between two point patterns. IEEE Trans. Pattern Anal. Mach. Intell., 13(4), 1991. 8 [118] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 5038‚Äì5047, 2017. 2, 13 [119] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. NeurIPS, 2017. 2, 4 [120] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 12 [121] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In CVPR, pages 14194‚Äì 14203, 2021. 7 [122] Jianyuan Wang, Yiran Zhong, Yuchao Dai, Stan Birchfield, Kaihao Zhang, Nikolai Smolyanskiy, and Hongdong Li. Deep two-view structure-from-motion revisited. In Pro- ceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 8953‚Äì8962, 2021. 2, 13 [123] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision, pages 9773‚Äì 9783, 2023. 13 [124] Jianyuan Wang, Christian Rupprecht, and David Novotny. PoseDiffusion: solving pose estimation via diffusion-aided bundle adjustment. In Proc. ICCV, 2023. 6, 7 [125] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. VGGSfM: visual geometry grounded deep structure from motion. In Proc. CVPR, 2024. 1, 2, 3, 6, 7, 10, 12, 13 [126] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. PF-LRM: pose-free large reconstruction model for joint pose and shape prediction. arXiv.cs, abs/2311.12024, 2023. 9 [127] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state, 2025. 2, 7 [128] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xi- ang, Yu Deng, Xin Tong, and Jiaolong Yang. MoGe: un- locking accurate monocular geometry estimation for open- domain images with optimal training supervision. arXiv, 2410.19115, 2024. 2 [129] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: Geometric 3D vision made easy. In Proc. CVPR, 2024. 1, 2, 3, 4, 6, 7, 11, 12, 13 [130] Yuesong Wang, Zhaojie Zeng, Tao Guan, Wei Yang, Zhuo Chen, Wenkai Liu, Luoyuan Xu, and Yawei Luo. Adap- tive patch deformation for textureless-resilient multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1621‚Äì1630, 2023. 2 [131] Xingkui Wei, Yinda Zhang, Zhuwen Li, Yanwei Fu, and Xiangyang Xue. Deepsfm: Structure from motion via deep bundle adjustment. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part I 16, pages 230‚Äì247. Springer, 2020. 2, 13 [132] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zex- iang Xu. MeshLRM: large reconstruction model for high- quality mesh. arXiv, 2404.12385, 2024. 4 18

--- ÌéòÏù¥ÏßÄ 19 ---
[133] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neu- ral radiance fields for indoor multi-view stereo. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision (ICCV), pages 5610‚Äì5619, 2021. 2 [134] Changchang Wu. Towards linear-time incremental struc- ture from motion. In 2013 International Conference on 3D Vision-3DV 2013, pages 127‚Äì134. IEEE, 2013. 2, 13 [135] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos, 2024. 6 [136] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20406‚Äì20417, 2024. 9 [137] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transform- ers. Advances in neural information processing systems, 34:12077‚Äì12090, 2021. 12 [138] Fei Xie, Chunyu Wang, Guangting Wang, Yue Cao, Wankou Yang, and Wenjun Zeng. Correlation-aware deep tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8751‚Äì8760, 2022. 13 [139] Qingshan Xu and Wenbing Tao. Learning inverse depth re- gression for multi-view stereo with correlation cost volume. In AAAI, 2020. 7 [140] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wet- zstein. GRM: Large gaussian reconstruction model for effi- cient 3D reconstruction and generation. arXiv, 2403.14621, 2024. 9 [141] Jianing Yang, Alexander Sax, Kevin J Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. 2, 7, 10 [142] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Ji- ashi Feng, and Hengshuang Zhao. Depth anything: Un- leashing the power of large-scale unlabeled data. In Proc. CVPR, 2024. 2 [143] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao- gang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any- thing v2. arXiv:2406.09414, 2024. 11 [144] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi- view stereo. In ECCV, 2018. 7 [145] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi- view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767‚Äì783, 2018. 2 [146] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo net- works. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1790‚Äì1799, 2020. 6 [147] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu- ral surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems, 33:2492‚Äì2502, 2020. 2 [148] Gokul Yenduri, Ramalingam M, Chemmalar Selvi G., Supriya Y, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H. Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. Generative pre-trained trans- former: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future di- rections. arXiv.cs, abs/2305.10435, 2023. 2 [149] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned Invariant Feature Transform. In Proc. ECCV, 2016. 2 [150] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. Stabilizing transformer training by pre- venting attention entropy collapse. In International Confer- ence on Machine Learning, pages 40770‚Äì40803. PMLR, 2023. 13 [151] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam- pani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. MonST3R: a simple approach for estimating geometry in the presence of motion. arXiv, 2410.03825, 2024. 11 [152] Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Relpose: Predicting probabilistic relative rotation for sin- gle objects in the wild. In ECCV, pages 592‚Äì611. Springer, 2022. 13 [153] Jason Y Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In International Conference on Learning Representations (ICLR), 2024. 13 [154] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 1‚Äì19. Springer, 2024. 9 [155] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. GS-LRM: large reconstruction model for 3D Gaussian splatting. arXiv, 2404.19702, 2024. 9 [156] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gor- don Wetzstein. Flare: Feed-forward geometry, appear- ance and camera estimation from uncalibrated sparse views, 2025. 2, 7 [157] Zhe Zhang, Rui Peng, Yuxi Hu, and Ronggang Wang. Ge- omvsnet: Learning multi-view stereo with geometry per- ception. In CVPR, 2023. 2, 7 [158] Xiaoming Zhao, Xingming Wu, Weihai Chen, Peter CY Chen, Qingsong Xu, and Zhengguo Li. Aliked: A lighter keypoint and descriptor extraction network via deformable 19

--- ÌéòÏù¥ÏßÄ 20 ---
transformation. IEEE Transactions on Instrumentation and Measurement, 72:1‚Äì16, 2023. 8 [159] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wet- zstein, and Leonidas J. Guibas. Pointodyssey: A large-scale synthetic dataset for long-term point tracking. In ICCV, 2023. 6 [160] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego- motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1851‚Äì 1858, 2017. 2, 13 [161] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 6, 7 20
